\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{lmodern}
\usepackage{mathtools, nccmath}
\usepackage{xparse}
\usepackage{colonequals}

%set mapping
\pagestyle{empty}
\usepackage{tikz}
\usetikzlibrary{calc,trees,positioning,arrows,fit,shapes,calc}

\usepackage{hyperref}          % clickable URls and cross-references
\hypersetup{
	colorlinks=true,
	urlcolor={blue!80!black},
    pdfborderstyle={/S/U/W 1}, % underline links instead of boxes
    linkbordercolor=red,       % color of internal links
    citebordercolor=green,     % color of links to bibliography
    filebordercolor=magenta,   % color of file links
    urlbordercolor=blue        % color of external links
}

\begin{document}
\title{Word Embedding}
\author{L.J. Brown}
\maketitle

%\newcommand\mat[1]{\mathcal{#1}}
%\newcommand\mat[1]{\boldsymbol{\mathcal{#1}}}
%\newcommand\mat[1]{\boldsymbol{#1}}
%\newcommand\mat[3]{\boldsymbol{\mathcal{#1}}_{{#2} \times {#3}}}
%\newcommand\mate[1]{\boldsymbol{\mathcal{#1}}}
\newcommand\mat[3]{\boldsymbol{{#1}}_{{#2} \times {#3}}}
\newcommand\mate[1]{\boldsymbol{#1}}

\newcommand{\N}{\mathbb N}
\newcommand{\Q}{\mathbb Q}

\newcommand*{\logeq}{\ratio\Leftrightarrow}

\DeclarePairedDelimiterX{\set}[1]{\{}{\}}{\setargs{#1}}
\NewDocumentCommand{\setargs}{>{\SplitArgument{1}{;}}m}
{\setargsaux#1}
\NewDocumentCommand{\setargsaux}{mm}
{\IfNoValueTF{#2}{#1} {#1\,\delimsize|\,\mathopen{}#2}}%{#1\:;\:#2}

%
%	Abstract
%

\section{Introduction}

This paper outlines a few methods used in this repository to map all unique words found in a corpus to vectors in a continuous vector space. The idea, and hope, is that some relationships between words found in the corpus will be preserved through this mapping and will manifest as characteristics of the vector space. Successful implimentations and strides in this area include \href{https://en.wikipedia.org/wiki/Latent_Semantic_Analysis}{Latent Semantic Analysis}, \href{https://en.wikipedia.org/wiki/Word2vec}{"Word2vec"}, and \href{https://nlp.stanford.edu/projects/glove/}{"GloVe"}.

%
%	Overview of General Strategy
%

\section{General Strategy}
The essential steps taken by each implementation within this repository are as follows: 

\begin{enumerate}
	\item 
		\textbf{Construct a Cooccurrence Matrix}\\
		Build a square "Cooccurrence Matrix", $\mate{A}$, where each element roughly represents some function, $f$, of the frequency or probability that two words, $w_i$ and $w_j$, occur together in the corpus (the diagonal elements being associated with the same word twice).
		
		%Cooccurrence Matrix
		\begin{equation*}
		\mat{A}{n}{n}=
		\bordermatrix{	&	w_1			&	w_2			& \ldots &	w_n			\cr
                w_1 	& f(w_1,w_1) 	&  f(w_1,w_2)   & \ldots & f(w_1,w_n) 	\cr
                w_2 	& f(w_2,w_1) 	&  f(w_2,w_2) 	& \ldots & f(w_2,w_n) 	\cr
                \vdots	& \vdots 		& \vdots 		& \ddots & \vdots		\cr
                w_n		& f(w_n,w_1)  	&  f(w_n,w_2)   & \ldots & f(w_n,w_n) 	}
		\end{equation*}
	\item 
		\textbf{Decompose the Cooccurrence Matrix into unique Word Vectors}\\
		Search for word vectors with the soft constraint that given any word vector pair, their inner product will yield a value close to the two values in the co-occurrence matrix associated with those two words. This constraint can be written as
		
		%constraint and word vectors
		\begin{equation*}
			\vec{w}_{i} \cdot \vec{w}_{j}^T \approx f(w_i,w_j)
		\end{equation*}
		
		If the desired word vectors, $\left( \vec{w}_{1}, \ldots, \vec{w}_{n} \right)$, are written as the columns of some "Word Vector Matrix", $\mate{W}$, then this constraint can be written as
		
		%constraint and word vector matrix
		\begin{equation*}
			\mate{W}\mate{W}^T \approx \mate{A}
		\end{equation*}
		
		Several methods to perform this decomposition step are explored within this repository. 
\end{enumerate}

%
%	Overview of Decomposition Methods
%

\section{Cooccurrence Matrix Decomposition Methods}
	The methods implimented in this repository to decompose the Cooccurrence Matrix, $\mate{A}$, into $\mate{W}\mate{W}^T$ are
	\begin{enumerate}
		\item \textbf{Stochastic Gradient Descent}\\
			An iterative optimization method used to minimize an objective function. Stochastic Gradient Descent (SGD) is often used over Batch Gradient Decent (BGD) for large noisy datasets.
			Our implementation of SGD Draws heavily on the implementations by "Word2vec" and "GloVe".
			
		\item \textbf{Eigen Decomposition of a forced Symmetric Positive Matrix}\\
			First the Cooccurrence Matrix, $\mate{A}$, is updated to have positive Eigen values. This is achieved by modifying the unused diagonal entries to ensure diagonal dominance. The new real symmetric matrix, $\tilde{\mate{A}}$, can be written as $\mate{Q}\mate{\Lambda}\mate{Q}^T$, and because of the positive Eigen values, $\mate{W} = \mate{Q} \sqrt{\mate{\Lambda}}$ will satisfy the constraint.
			
		\item \textbf{Singular Value Decomposition of a forced Symmetric Positive Definite Matrix}\\
			Similarly to the EVD method, the Cooccurrence Matrix, $\mate{A}$, is updated, but now to be positive definite. This is again achieved by modifying the unused diagonal entries to ensure diagonal dominance with the additional constraint that all diagonal entries are greater than zero. The new positive definite matrix, $\tilde{\mate{A}}$, can be written as $\mate{U}\mate{\Sigma}\mate{V}^T$, and because it is symmetric and positive definite, $\mate{U} = \mate{V}$, and  $\mate{W} = \mate{V} \sqrt{\mate{\Lambda}}$ will satisfy the constraint.
	\end{enumerate}

%
%	Detailed Construction of Cooccurrence Matrix
%

\section{Details of Cooccurrence Matrix}

%Defining Cooccurrence Matrix A
This section details how this repository defines it's $n \times n$ cooccurrence matrix, $\mat{A}{n}{n}$, whose rows and columns correspond to the set of unique words, $W$, and whose elements are the output of a cooccurrence function, $f(w_i, w_j)$, which takes words as inputs. 

For clarity it is useful to define a function, $m(w_i) = i$, that converts unique words to unique integers in the range $1$ to $n$ and and its inverse, $m^{-1}(i) = w_i$ , which does the opposite.

	%definitions
	\begin{equation*}
	n \equiv \text{Number of unique words in corpus, } W \equiv \text{Set of unique words in corpus}
	\end{equation*}
	\begin{equation*}
	W = \set {w_1,\ldots ,w_n} \text{, } \, I = \set {1,\,\ldots ,\,n}
	\end{equation*}
	
	%mapping equation m : W <-> I
	\begin{equation*}
	m: W \rightarrow I \text{, } \, m^{-1}: W \leftarrow I
	\end{equation*}
	
	%mapping equation m : W <-> I
	\begin{equation*}
	f(w_i,w_j) \equiv \text{Cooccurrence function defined bellow}
	\end{equation*}

%Defining Cooccurrence Matrix A
\begin{equation*}
\mat{A}{n}{n}=
\bordermatrix{			&	c_1				&	c_2		&	\ldots 	&	c_n			\cr
                r_1		&	a_{11}			&  a_{12} 	& 	\ldots 	& 	a_{1n}		\cr
                r_2		&	a_{21} 			&  a_{22}	& 	\ldots 	& 	a_{2n}		\cr
                \vdots	& 	\vdots 			& 	\vdots 	& 	\ddots 	& 	\vdots		\cr
                r_n		&	a_{n1}			&  a_{n2}   &	\ldots 	& 	a_{nn}}
\end{equation*}

\begin{equation*}
\text{where } a_{ij} = f(m^{-1}(r_i),m^{-1}(c_j)) = f(w_i,w_j)
\end{equation*}

For each element, $a_{ij}$, the words used as inputs for $f$ are the words whose integer mappings correspond to the elements row, $r_i$, and column, $c_j$.


%defining the cooccurrence function, $f(w_i, w_j)$
In order to define the cooccurrence function, $f$, it is useful to introduce some new variables. For each of the $m$ sentences in the corpus we define $s_i$ as the sequence of words in that particular sentence. Where $S$ is a \textbf{sequence} containing the $m$ \textbf{sequences}, $\left(s_1, \ldots, s_m \right)$, corresponding to the $m$ sentences of the corpus.

\begin{equation*}
S = \left( s_1, \ldots, s_m \right)
\end{equation*}

For example if the $i^{\text{th}}$ sentence in the corpus is: ``God made mud.'', then the corresponding sequence, $s_i$, in $S$ would be

\begin{equation*}
s_i = \left( \tilde{w_1}, \tilde{w_2}, \tilde{w_3} \right)
\end{equation*}
\begin{equation*}
\text{where } \tilde{w_1}, \tilde{w_2}, \tilde{w_3} \, \epsilon \, W
\end{equation*}

Also used in the construction of the concurrence matrix is a distance function, $d(s_i, \set{\tilde{w_j}, \tilde{w_k}})$, whose parameters are a sequence of words, $s_i$, in $S$ (corresponding to a sentence of the corpus) and a set or an unordered pair of words that are members of the sequence $s_i$. For example using the $i^{\text{th}}$ sentence in the corpus again and a random ordered pair from that sequence $\set{\tilde{w_2}, \tilde{w_3}} = \set{"made", "mud"}$

\begin{equation*}
d(s_i, \set{\tilde{w_2}, \tilde{w_3}}) = d(\set{"God", "made", "mud"}, \set{\text{"made"}, \text{"mud"}})
\end{equation*}


It is important to note that although $\tilde{w_2}$, $\tilde{w_3} \, \epsilon \, W$ it is not necessarily true that $\tilde{w_2} = w_2$ or $\tilde{w_3} = w_3$. The distance function, $d(s_i, \set{\tilde{w_j}, \tilde{w_k}})$, returns a value one more than the number of words between the word pair, $\set{\tilde{w_j}, \tilde{w_k}}$, in sentence corresponding to the sequence, $s_i$.

\begin{equation*}
d(s_i, \set{\tilde{w_j}, \tilde{w_k}}) = \mid j - k \mid
\end{equation*}

One more useful definition due to obscure notation, $\set{[s_i]^2}$ is the set of all unordered pairs of words in the sequence $s_i$. Using the example above for $s_i$

\begin{equation*}
\set{[s_i]^2} = \set{\set{\text{"God"}, \text{"made"}}, \set{\text{"God"}, \text{"mud"}}, \set{\text{"made"}, \text{"mud"}} }
\end{equation*}

Finally the cooccurrence function, $f(w_i, w_j)$, is defined as

\begin{equation*}
f(w_i,w_j) = 
\sum_{s \epsilon S} { \sum_{p \epsilon \set{[s]^2} } {
\begin{cases}
\log \left(\frac{1}{d(s,p)} \right), \, \text{if} \, w_i, w_j \, \epsilon \, p \, \text{and if} \, w_i \neq w_j
\\
0, \, \text{otherwise}
\end{cases}
} }
\end{equation*}

Stated simply, $f(w_i, w_j)$ finds all times that the words $w_i$ and $w_j$ appear together in sentences of the corpus, and sums the $\log \left( \frac{1}{\text{their distance apart}} \right)$. Note that the value of the cooccurrence function, $f(w_i, w_j)$, when the two inputs are the same word is zero. This corresponds to the diagonal entries of the cooccurrence matrix $\mate{A}$. The matrix $\mate{A}$ will also be symmetric due to the property of $f(w_i, w_j)$ that,

\begin{equation*}
f(w_i,w_j) = f(w_j,w_i)
\end{equation*}
therefore,
\begin{equation*}
a_{ij} = a_{ji},
\end{equation*}

\begin{equation*}
\mate{A} = \mate{A}^T
\end{equation*}


%
%	Detailed Definition of Word Vectors and Word Vector Matrix
%

\section{Details of Word Vectors and Word Vector Matrix}
%defining n word vectors of dimensionality d
%Defining soft constraint of equality
Next we search for a set of $n$ word vectors, $\vec{w}_{i},\ldots ,\vec{w}_{n}$ , of dimensionality $v$ that best satisfy the condition that the inner product between any two word vectors is a value close to the two elements in matrix $\mate{A}$ corresponding to the two words intersection. We exclude the diagonal elements from meeting this condition which correspond to the intersection of one word with itself. We will represent the desired $n$ word vectors as the column vectors of a matrix $\mat{W}{v}{n}$.

  \begin{align*}
    \mat{W}{v}{n} &= \begin{pmatrix}
    	\begin{bmatrix} \vec{w}_{1} \end{bmatrix} , \ldots , \begin{bmatrix} \vec{w}_{n}\end{bmatrix}
	 \end{pmatrix}
  \end{align*}

In order to formally define the properties of the word vectors we wish to find, we write a set of $\frac{n^2 - n}{2}$ soft constraints, $C = \set{c_{12},c_{13},\cdots ,c_{n \choose 2}}$, as
\begin{equation*}
c_{ij} \equiv \vec{w}_{i}^T \cdot \vec{w}_{j} \logeq a_{ij} \text{, where $i \neq j$.}
\end{equation*}

\begin{equation*}
\text{where } \frac{n^2 - n}{2} \text{ is the number of non-diagonal elements in the matrix } \mate{A}.
\end{equation*}
However, order does not matter for the constraints when taking the inner product of two word vectors as $\mate{A}$ is symmetric about its diagonal and 

\begin{equation*}
\vec{w}_{i}^T \cdot \vec{w}_{j} = \vec{w}_{j}^T \cdot \vec{w}_{i}
\end{equation*}

\begin{equation*}
a_{ij} = a_{ji}
\end{equation*}


%Error function between two word vectors
\text{We define an error function, $\varepsilon$, between two word vectors, $\vec{w}_{i}$ and $\vec{w}_{j}$, as}
\begin{equation*}
\varepsilon(i,j) = \vec{w}_{i}^T \cdot \vec{w}_{j} - a_{ij}
\end{equation*}
\begin{equation*}
\text{where $a_{ij}$ is an element in the Matrix $\mate{A}$}
\end{equation*}

%Objective function J for entire dataset
The method we choose to find the values of the matrix $\mate{W}$ is \textbf{Stochastic Gradient Descent}. We define an objective function, $J$, to minimize as
\begin{equation*}
J = \frac{1}{2 {n \choose 2} } \sum_{r=1}^{n}{\sum_{c=r+1}^{n}{
\begin{cases}
\varepsilon(r,c)^2, \, if \, r \neq c
\\
0, \, otherwise
\end{cases}
}}
\end{equation*}

When $J$ is at a minimum then the column vectors of $\mate{W}$ will best meet our set of soft constraints, $C$.

%Defining an objective function for a specific word vector
We next define an individual objective function for each word vector, $J_{\vec{w}_{i}}$. During the optimization process, and during each iteration, we randomly select a column vector of $\mate{W}$, $\vec{w_i}$, to update by computing the gradient of the chosen individual word vector, $\frac{\partial J_{\vec{w}_{i}}}{\partial \vec{w}_{i}}$. We define the objective function for an individual word vector, $\vec{w}_{i}$,  as
\begin{equation*}
J_{\vec{w}_{i}} = \frac{1}{2} \sum_{j=1}^{n} {\begin{cases}
\varepsilon(i,j)^2, \, if \, i \neq j
\\
0, \, otherwise
\end{cases}}
\end{equation*}

And compute the gradient of that specific objective function, $\frac{\partial J_{\vec{w}_{i}}}{\partial \vec{w}_{i}}$, as
%Defining the gradient for an individual word vectors objective function
\begin{equation*}
\frac{\partial J_{\vec{w}_{i}}}{\partial \vec{w}_{i}} = \sum_{j=1}^{n}{\begin{cases}
\vec{w}_{i} \, \odot \, \left( \vec{w}_{i}^T \cdot \vec{w}_{j} - y_{ij} \right), \, if \, i \neq j
\\
0, \, otherwise
\end{cases}}
\end{equation*}

%iteration updation for random word vector
The optimization process in pseudo code is outlined bellow:
$\\\\ n \equiv \text{Number of unique words in corpus}$
$\\ m \equiv \text{Number of training iterations}$
$\\ \eta \equiv \text{Learning rate}$

\begin{equation*}
\text{For $i = 1,2, \ldots , m$, do:}
\end{equation*}
\begin{equation*}
x := \text{random number range} [1,n]
\end{equation*}
\begin{equation*}
\vec{w}_{x} := \vec{w}_{x} - \eta \frac{\partial J_{\vec{w}_{x}}}{\partial \vec{w}_{x}}
\end{equation*}


%
%	Method of Symmetric Positive Definite Decomposition
%

The conditions we set for the set of word vectors can be stated in another way, our goal is to decompose the square, real, symmetric, cooccurrence matrix $\mat{A}{n}{n}$ into a word vector matrix, $\mate{W}$, multiplied by its transpose. This would satisfies the conditions that the inner product between any two word vectors is equal to the two elements in matrix $\mate{A}$ corresponding to the two words intersection.

\begin{equation*}
\mate{A} = \mate{W}\mate{W}^T
\end{equation*}

Real symmetric matrices can be decomposed into the form

\begin{equation*}
\mate{A} = \mate{Q}\mate{\Lambda}\mate{Q}^T
\end{equation*}

Where $\mate{Q}$ is an orthogonal matrix and $\mate{\Lambda}$ is a diagonal matrix whose entries are the eigenvalues of $\mate{A}$. If the eigenvalues of $\mate{A}$ are all positive then we can write as

\begin{equation*}
\mate{W} = \mate{Q} \sqrt{\mate{\Lambda}}
\end{equation*}

\begin{equation*}
\mate{A} = \mate{Q}\mate{\Lambda}\mate{Q}^T =  \mate{Q} \sqrt{\mate{\Lambda}} \sqrt{\mate{\Lambda}}^T \mate{Q}^T = \mate{Q} \sqrt{\mate{\Lambda}} \left( \mate{Q} \sqrt{\mate{\Lambda}} \right)^T = \mate{W}\mate{W}^T
\end{equation*}

\begin{equation*}
\text{$\mate{\Lambda} = \mate{\Lambda}^T$ and $\sqrt{\mate{\Lambda}} = \sqrt{\mate{\Lambda}}^T$}
\end{equation*}

However, it is not guaranteed that the eigenvalues of $\mate{A}$ will all be positive. But we can force this to be the case by making use of the free diagonals of the cooccurrence matrix $\mate{A}$ and making it diagonally dominant.

\begin{equation*}
\text{A matrix is diagonally dominant if }
\lvert \mate{a}_{ii} \rvert \geq \sum_{j \neq i}{\lvert \mate{a}_{ij} \rvert} \, \text{for all } \, i \text{.}
\end{equation*}

If we add the condition that $\mate{a}_{ii} > 0$ for all $i$, then this matrix will also be positive definite and we can use singular value decomposition to compute $\tilde{\mate{A}} = \mate{W}\mate{W}^T$. If $\tilde{\mate{A}}$ is a symmetric positive definite matrix then by the spectral theorem we know that $\mate{\Sigma} = \mate{\Lambda}$ and $\mate{U} = \mate{V} = \mate{Q}$.


\begin{equation*}
\tilde{\mate{A}} = \mate{U}\mate{\Sigma}\mate{V}^T
\end{equation*}

\begin{equation*}
\tilde{\mate{A}}^T\tilde{\mate{A}} = \tilde{\mate{A}}\tilde{\mate{A}}^T  \text{, therefore } \, \mate{U} = \mate{V}
\end{equation*}


\begin{equation*}
\text{And we can write } \, \tilde{\mate{A}} \, \text{as } \, \mate{W}\mate{W}^T \, \text{where } \, \mate{W} = \mate{V} \sqrt{\mate{\Lambda}}
\end{equation*}


\end{document}